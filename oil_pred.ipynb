{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      ANN MODEL FOR PREDICTION OF BUBBLE POINT\n",
    "#                          PRESSURE OF CRUDE OILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL LIBRARIES USED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------EXCEL DATA---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Simulation Data (1).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOR</th>\n",
       "      <th>Oil gravity</th>\n",
       "      <th>Gas gravity</th>\n",
       "      <th>T</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1507</td>\n",
       "      <td>0.951</td>\n",
       "      <td>39.3</td>\n",
       "      <td>225</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898</td>\n",
       "      <td>0.802</td>\n",
       "      <td>32.7</td>\n",
       "      <td>175</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>1.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>898</td>\n",
       "      <td>0.802</td>\n",
       "      <td>32.7</td>\n",
       "      <td>150</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>1.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.930</td>\n",
       "      <td>42.8</td>\n",
       "      <td>235</td>\n",
       "      <td>3405.0</td>\n",
       "      <td>1.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825</td>\n",
       "      <td>0.779</td>\n",
       "      <td>34.2</td>\n",
       "      <td>185</td>\n",
       "      <td>3354.0</td>\n",
       "      <td>1.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GOR  Oil gravity  Gas gravity    T      Pb    Bob\n",
       "0  1507        0.951         39.3  225  3573.0  1.875\n",
       "1   898        0.802         32.7  175  3571.0  1.471\n",
       "2   898        0.802         32.7  150  3426.0  1.451\n",
       "3  1579        0.930         42.8  235  3405.0  1.997\n",
       "4   825        0.779         34.2  185  3354.0  1.431"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------   SPLIT DATA FOR TRAIN AND TEST ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(data[['GOR', 'Oil gravity', 'Gas gravity', 'T']], data.Pb)\n",
    "ss=StandardScaler()\n",
    "X_train=ss.fit_transform(X_train)\n",
    "X_test=ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape no matter how many col it became 1\n",
    "y_train=y_train.values.reshape(-1,1)\n",
    "y_test=y_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32, shape=(None,4), name='X' )\n",
    "y=tf.placeholder(dtype=tf.float32, shape=(None,1), name='y' )\n",
    "\n",
    "# setup hidden layer. data, neuron act func name\n",
    "h1=tf.layers.dense(X,8, activation=tf.nn.relu, name='hidden1')\n",
    "\n",
    "# output layer\n",
    "\n",
    "output=tf.layers.dense(h1, 1, activation=None, name='output')\n",
    "\n",
    "loss=tf.losses.mean_squared_error(y, output)\n",
    "optimizer=tf.train.AdamOptimizer(.01)\n",
    "\n",
    "training_run=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- ITERATED 50 EPOCH ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 4388963.5 test loss 4397765.0\n",
      "epoch 1 train loss 4388692.5 test loss 4397495.0\n",
      "epoch 2 train loss 4388427.5 test loss 4397229.0\n",
      "epoch 3 train loss 4388164.0 test loss 4396966.0\n",
      "epoch 4 train loss 4387904.5 test loss 4396704.0\n",
      "epoch 5 train loss 4387647.0 test loss 4396446.0\n",
      "epoch 6 train loss 4387394.5 test loss 4396190.0\n",
      "epoch 7 train loss 4387142.0 test loss 4395933.0\n",
      "epoch 8 train loss 4386890.0 test loss 4395675.0\n",
      "epoch 9 train loss 4386637.0 test loss 4395416.0\n",
      "epoch 10 train loss 4386382.0 test loss 4395155.0\n",
      "epoch 11 train loss 4386124.0 test loss 4394893.0\n",
      "epoch 12 train loss 4385863.0 test loss 4394628.0\n",
      "epoch 13 train loss 4385597.5 test loss 4394360.0\n",
      "epoch 14 train loss 4385327.5 test loss 4394089.5\n",
      "epoch 15 train loss 4385054.0 test loss 4393818.0\n",
      "epoch 16 train loss 4384776.0 test loss 4393543.0\n",
      "epoch 17 train loss 4384494.5 test loss 4393263.0\n",
      "epoch 18 train loss 4384207.5 test loss 4392977.0\n",
      "epoch 19 train loss 4383913.5 test loss 4392685.5\n",
      "epoch 20 train loss 4383615.0 test loss 4392388.0\n",
      "epoch 21 train loss 4383310.0 test loss 4392085.0\n",
      "epoch 22 train loss 4382997.5 test loss 4391776.0\n",
      "epoch 23 train loss 4382678.0 test loss 4391461.0\n",
      "epoch 24 train loss 4382352.5 test loss 4391139.0\n",
      "epoch 25 train loss 4382019.0 test loss 4390809.0\n",
      "epoch 26 train loss 4381677.5 test loss 4390472.0\n",
      "epoch 27 train loss 4381329.5 test loss 4390127.5\n",
      "epoch 28 train loss 4380971.5 test loss 4389774.0\n",
      "epoch 29 train loss 4380605.5 test loss 4389413.0\n",
      "epoch 30 train loss 4380230.5 test loss 4389043.5\n",
      "epoch 31 train loss 4379846.0 test loss 4388665.0\n",
      "epoch 32 train loss 4379452.0 test loss 4388274.5\n",
      "epoch 33 train loss 4379048.5 test loss 4387872.5\n",
      "epoch 34 train loss 4378635.5 test loss 4387460.0\n",
      "epoch 35 train loss 4378212.5 test loss 4387037.5\n",
      "epoch 36 train loss 4377779.5 test loss 4386604.0\n",
      "epoch 37 train loss 4377337.0 test loss 4386160.0\n",
      "epoch 38 train loss 4376881.0 test loss 4385704.0\n",
      "epoch 39 train loss 4376414.0 test loss 4385236.0\n",
      "epoch 40 train loss 4375936.5 test loss 4384756.0\n",
      "epoch 41 train loss 4375446.5 test loss 4384265.0\n",
      "epoch 42 train loss 4374945.0 test loss 4383763.0\n",
      "epoch 43 train loss 4374432.0 test loss 4383248.0\n",
      "epoch 44 train loss 4373907.5 test loss 4382721.0\n",
      "epoch 45 train loss 4373370.0 test loss 4382179.0\n",
      "epoch 46 train loss 4372821.5 test loss 4381624.0\n",
      "epoch 47 train loss 4372261.0 test loss 4381057.0\n",
      "epoch 48 train loss 4371685.0 test loss 4380478.0\n",
      "epoch 49 train loss 4371096.5 test loss 4379886.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------5000 epoch iteration ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 4390502.0 test loss 4399422.0\n",
      "epoch 10 train loss 4387767.0 test loss 4396779.0\n",
      "epoch 20 train loss 4385085.5 test loss 4394196.0\n",
      "epoch 30 train loss 4382127.0 test loss 4391328.0\n",
      "epoch 40 train loss 4378543.0 test loss 4387844.5\n",
      "epoch 50 train loss 4374052.5 test loss 4383474.0\n",
      "epoch 60 train loss 4368535.5 test loss 4378070.0\n",
      "epoch 70 train loss 4361903.0 test loss 4371526.0\n",
      "epoch 80 train loss 4354100.0 test loss 4363815.0\n",
      "epoch 90 train loss 4345090.0 test loss 4354881.0\n",
      "epoch 100 train loss 4334840.0 test loss 4344701.0\n",
      "epoch 110 train loss 4323328.5 test loss 4333265.5\n",
      "epoch 120 train loss 4310547.0 test loss 4320555.5\n",
      "epoch 130 train loss 4296446.0 test loss 4306536.0\n",
      "epoch 140 train loss 4280938.5 test loss 4291091.0\n",
      "epoch 150 train loss 4263824.0 test loss 4274016.0\n",
      "epoch 160 train loss 4244774.5 test loss 4255029.0\n",
      "epoch 170 train loss 4223383.0 test loss 4233829.5\n",
      "epoch 180 train loss 4199724.0 test loss 4210358.0\n",
      "epoch 190 train loss 4174018.2 test loss 4184768.2\n",
      "epoch 200 train loss 4146436.0 test loss 4157188.0\n",
      "epoch 210 train loss 4117066.8 test loss 4127756.0\n",
      "epoch 220 train loss 4086015.2 test loss 4096569.5\n",
      "epoch 230 train loss 4053429.8 test loss 4063751.2\n",
      "epoch 240 train loss 4019407.2 test loss 4029455.5\n",
      "epoch 250 train loss 3984022.8 test loss 3993758.0\n",
      "epoch 260 train loss 3947382.8 test loss 3956714.5\n",
      "epoch 270 train loss 3909561.0 test loss 3918375.0\n",
      "epoch 280 train loss 3870637.5 test loss 3878863.0\n",
      "epoch 290 train loss 3830717.8 test loss 3838272.5\n",
      "epoch 300 train loss 3789864.0 test loss 3796669.8\n",
      "epoch 310 train loss 3748160.8 test loss 3754082.0\n",
      "epoch 320 train loss 3705666.2 test loss 3710581.0\n",
      "epoch 330 train loss 3662416.0 test loss 3666253.8\n",
      "epoch 340 train loss 3618500.8 test loss 3621165.0\n",
      "epoch 350 train loss 3573967.0 test loss 3575366.8\n",
      "epoch 360 train loss 3528874.2 test loss 3528933.5\n",
      "epoch 370 train loss 3483295.5 test loss 3481929.5\n",
      "epoch 380 train loss 3437283.0 test loss 3434382.5\n",
      "epoch 390 train loss 3390845.0 test loss 3386370.5\n",
      "epoch 400 train loss 3344048.5 test loss 3337946.0\n",
      "epoch 410 train loss 3296928.8 test loss 3289165.5\n",
      "epoch 420 train loss 3249527.8 test loss 3240079.5\n",
      "epoch 430 train loss 3201891.0 test loss 3190739.0\n",
      "epoch 440 train loss 3154053.8 test loss 3141177.5\n",
      "epoch 450 train loss 3106050.8 test loss 3091459.0\n",
      "epoch 460 train loss 3057902.8 test loss 3041601.0\n",
      "epoch 470 train loss 3009637.0 test loss 2991647.0\n",
      "epoch 480 train loss 2961280.8 test loss 2941618.5\n",
      "epoch 490 train loss 2912865.5 test loss 2891579.5\n",
      "epoch 500 train loss 2864417.2 test loss 2841517.0\n",
      "epoch 510 train loss 2815976.0 test loss 2791530.0\n",
      "epoch 520 train loss 2767580.8 test loss 2741660.5\n",
      "epoch 530 train loss 2719265.8 test loss 2691934.0\n",
      "epoch 540 train loss 2671052.2 test loss 2642387.2\n",
      "epoch 550 train loss 2622965.2 test loss 2593030.0\n",
      "epoch 560 train loss 2575049.2 test loss 2543918.0\n",
      "epoch 570 train loss 2527349.8 test loss 2495094.0\n",
      "epoch 580 train loss 2479897.2 test loss 2446568.5\n",
      "epoch 590 train loss 2432732.8 test loss 2398271.0\n",
      "epoch 600 train loss 2385878.0 test loss 2350211.2\n",
      "epoch 610 train loss 2339301.0 test loss 2302486.2\n",
      "epoch 620 train loss 2292950.5 test loss 2254994.2\n",
      "epoch 630 train loss 2246856.5 test loss 2207765.2\n",
      "epoch 640 train loss 2201053.8 test loss 2160842.8\n",
      "epoch 650 train loss 2155594.8 test loss 2114282.8\n",
      "epoch 660 train loss 2110522.8 test loss 2068127.1\n",
      "epoch 670 train loss 2065904.2 test loss 2022433.9\n",
      "epoch 680 train loss 2021707.6 test loss 1977218.0\n",
      "epoch 690 train loss 1977982.5 test loss 1932488.5\n",
      "epoch 700 train loss 1934777.8 test loss 1888290.5\n",
      "epoch 710 train loss 1892121.2 test loss 1844653.0\n",
      "epoch 720 train loss 1850000.6 test loss 1801585.0\n",
      "epoch 730 train loss 1808436.9 test loss 1759050.8\n",
      "epoch 740 train loss 1767468.9 test loss 1717112.5\n",
      "epoch 750 train loss 1727117.2 test loss 1675800.2\n",
      "epoch 760 train loss 1687398.8 test loss 1635132.2\n",
      "epoch 770 train loss 1648329.6 test loss 1595077.8\n",
      "epoch 780 train loss 1609923.2 test loss 1555639.2\n",
      "epoch 790 train loss 1572184.2 test loss 1516885.8\n",
      "epoch 800 train loss 1535072.6 test loss 1478794.8\n",
      "epoch 810 train loss 1498641.2 test loss 1441350.2\n",
      "epoch 820 train loss 1462912.0 test loss 1404597.9\n",
      "epoch 830 train loss 1427879.6 test loss 1368570.0\n",
      "epoch 840 train loss 1393481.2 test loss 1333230.9\n",
      "epoch 850 train loss 1359631.4 test loss 1298508.4\n",
      "epoch 860 train loss 1326439.2 test loss 1264436.5\n",
      "epoch 870 train loss 1293819.4 test loss 1231037.0\n",
      "epoch 880 train loss 1261810.0 test loss 1198270.9\n",
      "epoch 890 train loss 1230529.6 test loss 1166248.0\n",
      "epoch 900 train loss 1200010.0 test loss 1135017.0\n",
      "epoch 910 train loss 1170258.0 test loss 1104596.0\n",
      "epoch 920 train loss 1141254.8 test loss 1074972.2\n",
      "epoch 930 train loss 1113027.4 test loss 1046158.75\n",
      "epoch 940 train loss 1085548.1 test loss 1018148.75\n",
      "epoch 950 train loss 1058844.1 test loss 990947.25\n",
      "epoch 960 train loss 1032921.06 test loss 964565.4\n",
      "epoch 970 train loss 1007779.1 test loss 938994.0\n",
      "epoch 980 train loss 983409.4 test loss 914208.0\n",
      "epoch 990 train loss 959754.2 test loss 890196.9\n",
      "epoch 1000 train loss 936849.94 test loss 866962.2\n",
      "epoch 1010 train loss 914678.25 test loss 844511.7\n",
      "epoch 1020 train loss 893107.4 test loss 822770.0\n",
      "epoch 1030 train loss 872102.1 test loss 801588.44\n",
      "epoch 1040 train loss 851790.5 test loss 781055.9\n",
      "epoch 1050 train loss 832185.6 test loss 761283.1\n",
      "epoch 1060 train loss 813157.75 test loss 742203.6\n",
      "epoch 1070 train loss 794686.6 test loss 723773.06\n",
      "epoch 1080 train loss 776813.8 test loss 705898.0\n",
      "epoch 1090 train loss 759351.4 test loss 688582.4\n",
      "epoch 1100 train loss 742269.4 test loss 671662.75\n",
      "epoch 1110 train loss 725665.5 test loss 655289.44\n",
      "epoch 1120 train loss 709592.5 test loss 639513.94\n",
      "epoch 1130 train loss 694086.9 test loss 624365.1\n",
      "epoch 1140 train loss 679033.9 test loss 609680.0\n",
      "epoch 1150 train loss 664515.2 test loss 595547.75\n",
      "epoch 1160 train loss 650568.9 test loss 582061.94\n",
      "epoch 1170 train loss 637104.94 test loss 569190.56\n",
      "epoch 1180 train loss 624206.1 test loss 556935.56\n",
      "epoch 1190 train loss 611889.8 test loss 545318.4\n",
      "epoch 1200 train loss 600090.56 test loss 534293.4\n",
      "epoch 1210 train loss 588792.75 test loss 523829.1\n",
      "epoch 1220 train loss 578033.1 test loss 513899.75\n",
      "epoch 1230 train loss 567810.9 test loss 504512.2\n",
      "epoch 1240 train loss 558107.6 test loss 495673.78\n",
      "epoch 1250 train loss 548901.2 test loss 487358.9\n",
      "epoch 1260 train loss 540136.44 test loss 479530.56\n",
      "epoch 1270 train loss 531796.06 test loss 472151.47\n",
      "epoch 1280 train loss 523829.53 test loss 465190.7\n",
      "epoch 1290 train loss 516259.84 test loss 458634.78\n",
      "epoch 1300 train loss 509079.03 test loss 452476.12\n",
      "epoch 1310 train loss 502271.12 test loss 446689.5\n",
      "epoch 1320 train loss 495818.03 test loss 441262.6\n",
      "epoch 1330 train loss 489692.75 test loss 436178.56\n",
      "epoch 1340 train loss 483853.03 test loss 431396.47\n",
      "epoch 1350 train loss 478308.75 test loss 426909.94\n",
      "epoch 1360 train loss 473050.16 test loss 422712.03\n",
      "epoch 1370 train loss 468063.3 test loss 418788.62\n",
      "epoch 1380 train loss 463333.25 test loss 415123.44\n",
      "epoch 1390 train loss 458830.0 test loss 411695.7\n",
      "epoch 1400 train loss 454534.1 test loss 408479.78\n",
      "epoch 1410 train loss 450431.88 test loss 405467.88\n",
      "epoch 1420 train loss 446515.3 test loss 402646.56\n",
      "epoch 1430 train loss 442763.06 test loss 399993.06\n",
      "epoch 1440 train loss 439183.56 test loss 397503.72\n",
      "epoch 1450 train loss 435759.72 test loss 395172.62\n",
      "epoch 1460 train loss 432480.44 test loss 392984.56\n",
      "epoch 1470 train loss 429330.97 test loss 390929.66\n",
      "epoch 1480 train loss 426316.4 test loss 389002.1\n",
      "epoch 1490 train loss 423431.84 test loss 387196.7\n",
      "epoch 1500 train loss 420669.88 test loss 385504.84\n",
      "epoch 1510 train loss 418023.28 test loss 383917.7\n",
      "epoch 1520 train loss 415450.78 test loss 382420.1\n",
      "epoch 1530 train loss 412966.94 test loss 381003.38\n",
      "epoch 1540 train loss 410574.3 test loss 379665.4\n",
      "epoch 1550 train loss 408269.72 test loss 378401.28\n",
      "epoch 1560 train loss 406048.16 test loss 377205.06\n",
      "epoch 1570 train loss 403904.7 test loss 376071.25\n",
      "epoch 1580 train loss 401826.53 test loss 374992.5\n",
      "epoch 1590 train loss 399780.16 test loss 373958.06\n",
      "epoch 1600 train loss 397788.56 test loss 372967.38\n",
      "epoch 1610 train loss 395855.25 test loss 372020.38\n",
      "epoch 1620 train loss 393978.72 test loss 371114.03\n",
      "epoch 1630 train loss 392142.2 test loss 370242.38\n",
      "epoch 1640 train loss 390308.66 test loss 369380.53\n",
      "epoch 1650 train loss 388506.0 test loss 368532.94\n",
      "epoch 1660 train loss 386741.34 test loss 367705.3\n",
      "epoch 1670 train loss 385015.4 test loss 366898.5\n",
      "epoch 1680 train loss 383327.16 test loss 366111.44\n",
      "epoch 1690 train loss 381674.88 test loss 365342.8\n",
      "epoch 1700 train loss 380056.8 test loss 364591.0\n",
      "epoch 1710 train loss 378471.06 test loss 363854.38\n",
      "epoch 1720 train loss 376916.1 test loss 363131.44\n",
      "epoch 1730 train loss 375383.06 test loss 362418.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1740 train loss 373858.4 test loss 361677.38\n",
      "epoch 1750 train loss 372355.44 test loss 360943.88\n",
      "epoch 1760 train loss 370876.3 test loss 360218.1\n",
      "epoch 1770 train loss 369401.06 test loss 359493.5\n",
      "epoch 1780 train loss 367906.06 test loss 358755.44\n",
      "epoch 1790 train loss 366408.0 test loss 357981.03\n",
      "epoch 1800 train loss 364925.12 test loss 357205.66\n",
      "epoch 1810 train loss 363460.94 test loss 356431.4\n",
      "epoch 1820 train loss 362016.34 test loss 355636.5\n",
      "epoch 1830 train loss 360584.25 test loss 354843.44\n",
      "epoch 1840 train loss 359160.25 test loss 354047.2\n",
      "epoch 1850 train loss 357752.28 test loss 353252.44\n",
      "epoch 1860 train loss 356351.78 test loss 352458.0\n",
      "epoch 1870 train loss 354926.16 test loss 351649.7\n",
      "epoch 1880 train loss 353488.28 test loss 350826.06\n",
      "epoch 1890 train loss 352059.78 test loss 349999.5\n",
      "epoch 1900 train loss 350645.62 test loss 349151.06\n",
      "epoch 1910 train loss 349247.28 test loss 348304.2\n",
      "epoch 1920 train loss 347864.88 test loss 347460.06\n",
      "epoch 1930 train loss 346486.62 test loss 346614.3\n",
      "epoch 1940 train loss 345119.34 test loss 345766.47\n",
      "epoch 1950 train loss 343766.25 test loss 344920.56\n",
      "epoch 1960 train loss 342427.72 test loss 344077.47\n",
      "epoch 1970 train loss 341103.8 test loss 343237.2\n",
      "epoch 1980 train loss 339794.2 test loss 342399.25\n",
      "epoch 1990 train loss 338498.72 test loss 341563.38\n",
      "epoch 2000 train loss 337200.0 test loss 340726.16\n",
      "epoch 2010 train loss 335883.1 test loss 339878.44\n",
      "epoch 2020 train loss 334550.03 test loss 339022.5\n",
      "epoch 2030 train loss 333222.53 test loss 338164.0\n",
      "epoch 2040 train loss 331905.8 test loss 337306.03\n",
      "epoch 2050 train loss 330601.6 test loss 336449.53\n",
      "epoch 2060 train loss 329310.34 test loss 335594.8\n",
      "epoch 2070 train loss 328031.88 test loss 334741.62\n",
      "epoch 2080 train loss 326766.1 test loss 333890.1\n",
      "epoch 2090 train loss 325512.8 test loss 333040.06\n",
      "epoch 2100 train loss 324271.84 test loss 332191.44\n",
      "epoch 2110 train loss 323039.25 test loss 331344.16\n",
      "epoch 2120 train loss 321793.34 test loss 330493.22\n",
      "epoch 2130 train loss 320543.53 test loss 329638.72\n",
      "epoch 2140 train loss 319301.38 test loss 328783.97\n",
      "epoch 2150 train loss 318066.47 test loss 327930.28\n",
      "epoch 2160 train loss 316827.97 test loss 327075.78\n",
      "epoch 2170 train loss 315597.94 test loss 326221.66\n",
      "epoch 2180 train loss 314379.1 test loss 325369.12\n",
      "epoch 2190 train loss 313172.0 test loss 324518.38\n",
      "epoch 2200 train loss 311968.38 test loss 323668.8\n",
      "epoch 2210 train loss 310767.78 test loss 322818.7\n",
      "epoch 2220 train loss 309546.06 test loss 321962.22\n",
      "epoch 2230 train loss 308311.4 test loss 321091.7\n",
      "epoch 2240 train loss 307083.25 test loss 320217.8\n",
      "epoch 2250 train loss 305865.72 test loss 319344.78\n",
      "epoch 2260 train loss 304660.16 test loss 318474.06\n",
      "epoch 2270 train loss 303466.94 test loss 317606.0\n",
      "epoch 2280 train loss 302286.12 test loss 316740.9\n",
      "epoch 2290 train loss 301117.66 test loss 315878.8\n",
      "epoch 2300 train loss 299961.44 test loss 315019.7\n",
      "epoch 2310 train loss 298804.34 test loss 314162.3\n",
      "epoch 2320 train loss 297617.34 test loss 313300.8\n",
      "epoch 2330 train loss 296404.75 test loss 312431.44\n",
      "epoch 2340 train loss 295197.12 test loss 311561.53\n",
      "epoch 2350 train loss 293992.44 test loss 310692.44\n",
      "epoch 2360 train loss 292784.5 test loss 309815.75\n",
      "epoch 2370 train loss 291545.3 test loss 308925.72\n",
      "epoch 2380 train loss 290221.22 test loss 307988.9\n",
      "epoch 2390 train loss 288841.03 test loss 306994.2\n",
      "epoch 2400 train loss 287462.12 test loss 305957.88\n",
      "epoch 2410 train loss 286083.72 test loss 304877.38\n",
      "epoch 2420 train loss 284687.12 test loss 303770.3\n",
      "epoch 2430 train loss 283296.84 test loss 302666.88\n",
      "epoch 2440 train loss 281914.6 test loss 301572.28\n",
      "epoch 2450 train loss 280538.9 test loss 300484.88\n",
      "epoch 2460 train loss 279171.12 test loss 299361.12\n",
      "epoch 2470 train loss 277809.66 test loss 298193.9\n",
      "epoch 2480 train loss 276461.75 test loss 297031.25\n",
      "epoch 2490 train loss 275134.0 test loss 295872.6\n",
      "epoch 2500 train loss 273827.78 test loss 294715.94\n",
      "epoch 2510 train loss 272531.7 test loss 293570.88\n",
      "epoch 2520 train loss 271245.03 test loss 292394.5\n",
      "epoch 2530 train loss 269977.47 test loss 291226.0\n",
      "epoch 2540 train loss 268729.75 test loss 290071.7\n",
      "epoch 2550 train loss 267477.38 test loss 288925.03\n",
      "epoch 2560 train loss 266194.38 test loss 287762.78\n",
      "epoch 2570 train loss 264879.12 test loss 286589.94\n",
      "epoch 2580 train loss 263561.72 test loss 285410.7\n",
      "epoch 2590 train loss 262213.47 test loss 284225.97\n",
      "epoch 2600 train loss 260860.11 test loss 283033.12\n",
      "epoch 2610 train loss 259516.25 test loss 281835.62\n",
      "epoch 2620 train loss 258073.25 test loss 280582.3\n",
      "epoch 2630 train loss 256595.33 test loss 279278.3\n",
      "epoch 2640 train loss 255128.95 test loss 277982.03\n",
      "epoch 2650 train loss 253680.52 test loss 276701.0\n",
      "epoch 2660 train loss 252258.77 test loss 275439.75\n",
      "epoch 2670 train loss 250865.44 test loss 274155.5\n",
      "epoch 2680 train loss 249499.1 test loss 272823.47\n",
      "epoch 2690 train loss 248126.08 test loss 271480.34\n",
      "epoch 2700 train loss 246641.92 test loss 270067.72\n",
      "epoch 2710 train loss 245042.19 test loss 268524.62\n",
      "epoch 2720 train loss 243432.0 test loss 266972.84\n",
      "epoch 2730 train loss 241766.94 test loss 265432.12\n",
      "epoch 2740 train loss 239996.06 test loss 263877.88\n",
      "epoch 2750 train loss 238077.39 test loss 262312.84\n",
      "epoch 2760 train loss 236080.89 test loss 260746.4\n",
      "epoch 2770 train loss 234107.78 test loss 259206.47\n",
      "epoch 2780 train loss 232175.89 test loss 257698.6\n",
      "epoch 2790 train loss 230252.7 test loss 256215.12\n",
      "epoch 2800 train loss 228176.61 test loss 254724.92\n",
      "epoch 2810 train loss 225965.69 test loss 253198.3\n",
      "epoch 2820 train loss 223758.7 test loss 251618.97\n",
      "epoch 2830 train loss 221562.47 test loss 250056.55\n",
      "epoch 2840 train loss 219224.78 test loss 248501.75\n",
      "epoch 2850 train loss 216763.88 test loss 246949.62\n",
      "epoch 2860 train loss 214158.14 test loss 245357.94\n",
      "epoch 2870 train loss 211463.45 test loss 243711.05\n",
      "epoch 2880 train loss 208788.89 test loss 241851.33\n",
      "epoch 2890 train loss 206155.72 test loss 239571.64\n",
      "epoch 2900 train loss 203592.06 test loss 237217.06\n",
      "epoch 2910 train loss 201084.05 test loss 234934.47\n",
      "epoch 2920 train loss 198640.42 test loss 232728.05\n",
      "epoch 2930 train loss 196032.95 test loss 230547.97\n",
      "epoch 2940 train loss 193323.0 test loss 228365.3\n",
      "epoch 2950 train loss 190649.03 test loss 226223.28\n",
      "epoch 2960 train loss 188023.92 test loss 224131.05\n",
      "epoch 2970 train loss 185472.47 test loss 222104.73\n",
      "epoch 2980 train loss 182898.92 test loss 220132.67\n",
      "epoch 2990 train loss 180266.2 test loss 218176.44\n",
      "epoch 3000 train loss 177671.16 test loss 216264.06\n",
      "epoch 3010 train loss 175139.5 test loss 214422.0\n",
      "epoch 3020 train loss 172693.23 test loss 212641.73\n",
      "epoch 3030 train loss 170324.58 test loss 210909.23\n",
      "epoch 3040 train loss 168061.44 test loss 209243.19\n",
      "epoch 3050 train loss 165888.61 test loss 207635.78\n",
      "epoch 3060 train loss 163816.02 test loss 206085.94\n",
      "epoch 3070 train loss 161849.03 test loss 204600.36\n",
      "epoch 3080 train loss 159982.38 test loss 203131.62\n",
      "epoch 3090 train loss 158210.02 test loss 201664.72\n",
      "epoch 3100 train loss 156526.6 test loss 200240.06\n",
      "epoch 3110 train loss 154927.84 test loss 198787.23\n",
      "epoch 3120 train loss 153409.2 test loss 197400.38\n",
      "epoch 3130 train loss 151966.2 test loss 196075.9\n",
      "epoch 3140 train loss 150594.61 test loss 194794.0\n",
      "epoch 3150 train loss 149290.39 test loss 193544.81\n",
      "epoch 3160 train loss 148055.23 test loss 192350.78\n",
      "epoch 3170 train loss 146882.47 test loss 191208.88\n",
      "epoch 3180 train loss 145767.16 test loss 190114.66\n",
      "epoch 3190 train loss 144706.08 test loss 189065.75\n",
      "epoch 3200 train loss 143696.0 test loss 188060.23\n",
      "epoch 3210 train loss 142726.98 test loss 187095.0\n",
      "epoch 3220 train loss 141794.7 test loss 186164.73\n",
      "epoch 3230 train loss 140908.8 test loss 185275.08\n",
      "epoch 3240 train loss 140065.58 test loss 184417.5\n",
      "epoch 3250 train loss 139263.92 test loss 183569.38\n",
      "epoch 3260 train loss 138500.52 test loss 182751.69\n",
      "epoch 3270 train loss 137762.03 test loss 181959.06\n",
      "epoch 3280 train loss 137055.38 test loss 181195.19\n",
      "epoch 3290 train loss 136373.23 test loss 180463.34\n",
      "epoch 3300 train loss 135710.94 test loss 179766.48\n",
      "epoch 3310 train loss 135074.44 test loss 179112.64\n",
      "epoch 3320 train loss 134463.9 test loss 178499.75\n",
      "epoch 3330 train loss 133879.11 test loss 177913.9\n",
      "epoch 3340 train loss 133322.16 test loss 177356.3\n",
      "epoch 3350 train loss 132790.9 test loss 176824.12\n",
      "epoch 3360 train loss 132281.55 test loss 176313.92\n",
      "epoch 3370 train loss 131792.22 test loss 175823.0\n",
      "epoch 3380 train loss 131319.45 test loss 175315.56\n",
      "epoch 3390 train loss 130859.71 test loss 174825.58\n",
      "epoch 3400 train loss 130415.664 test loss 174355.52\n",
      "epoch 3410 train loss 129982.84 test loss 173903.97\n",
      "epoch 3420 train loss 129558.59 test loss 173414.81\n",
      "epoch 3430 train loss 129147.11 test loss 172939.8\n",
      "epoch 3440 train loss 128748.48 test loss 172483.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3450 train loss 128363.68 test loss 172046.2\n",
      "epoch 3460 train loss 127991.56 test loss 171614.88\n",
      "epoch 3470 train loss 127631.61 test loss 171199.1\n",
      "epoch 3480 train loss 127282.875 test loss 170772.28\n",
      "epoch 3490 train loss 126946.73 test loss 170366.19\n",
      "epoch 3500 train loss 126620.75 test loss 169977.66\n",
      "epoch 3510 train loss 126302.78 test loss 169601.25\n",
      "epoch 3520 train loss 125989.234 test loss 169222.36\n",
      "epoch 3530 train loss 125686.14 test loss 168855.39\n",
      "epoch 3540 train loss 125397.26 test loss 168503.31\n",
      "epoch 3550 train loss 125117.17 test loss 168164.38\n",
      "epoch 3560 train loss 124841.84 test loss 167835.66\n",
      "epoch 3570 train loss 124565.43 test loss 167513.34\n",
      "epoch 3580 train loss 124288.51 test loss 167204.28\n",
      "epoch 3590 train loss 124002.51 test loss 166893.56\n",
      "epoch 3600 train loss 123714.086 test loss 166582.08\n",
      "epoch 3610 train loss 123428.98 test loss 166274.64\n",
      "epoch 3620 train loss 123139.62 test loss 165976.03\n",
      "epoch 3630 train loss 122850.17 test loss 165688.94\n",
      "epoch 3640 train loss 122557.16 test loss 165411.94\n",
      "epoch 3650 train loss 122266.305 test loss 165136.17\n",
      "epoch 3660 train loss 121979.27 test loss 164867.0\n",
      "epoch 3670 train loss 121696.37 test loss 164603.22\n",
      "epoch 3680 train loss 121417.62 test loss 164345.12\n",
      "epoch 3690 train loss 121142.81 test loss 164091.06\n",
      "epoch 3700 train loss 120871.83 test loss 163847.31\n",
      "epoch 3710 train loss 120604.47 test loss 163623.88\n",
      "epoch 3720 train loss 120340.6 test loss 163408.16\n",
      "epoch 3730 train loss 120072.945 test loss 163202.25\n",
      "epoch 3740 train loss 119797.91 test loss 163010.45\n",
      "epoch 3750 train loss 119520.03 test loss 162829.64\n",
      "epoch 3760 train loss 119243.68 test loss 162656.47\n",
      "epoch 3770 train loss 118969.99 test loss 162490.61\n",
      "epoch 3780 train loss 118695.61 test loss 162326.78\n",
      "epoch 3790 train loss 118423.23 test loss 162160.05\n",
      "epoch 3800 train loss 118152.47 test loss 161986.47\n",
      "epoch 3810 train loss 117879.91 test loss 161821.81\n",
      "epoch 3820 train loss 117609.02 test loss 161665.95\n",
      "epoch 3830 train loss 117340.62 test loss 161513.12\n",
      "epoch 3840 train loss 117074.89 test loss 161365.72\n",
      "epoch 3850 train loss 116811.86 test loss 161224.9\n",
      "epoch 3860 train loss 116528.5 test loss 161096.45\n",
      "epoch 3870 train loss 116219.53 test loss 160985.62\n",
      "epoch 3880 train loss 115905.97 test loss 160876.88\n",
      "epoch 3890 train loss 115594.96 test loss 160764.44\n",
      "epoch 3900 train loss 115291.26 test loss 160649.9\n",
      "epoch 3910 train loss 114988.56 test loss 160530.58\n",
      "epoch 3920 train loss 114686.6 test loss 160416.0\n",
      "epoch 3930 train loss 114373.336 test loss 160319.16\n",
      "epoch 3940 train loss 114061.664 test loss 160234.44\n",
      "epoch 3950 train loss 113752.99 test loss 160152.84\n",
      "epoch 3960 train loss 113447.695 test loss 160068.0\n",
      "epoch 3970 train loss 113145.695 test loss 159935.81\n",
      "epoch 3980 train loss 112822.32 test loss 159747.72\n",
      "epoch 3990 train loss 112384.69 test loss 159497.75\n",
      "epoch 4000 train loss 111928.93 test loss 159238.83\n",
      "epoch 4010 train loss 111500.86 test loss 159001.12\n",
      "epoch 4020 train loss 111090.445 test loss 158789.72\n",
      "epoch 4030 train loss 110695.01 test loss 158599.25\n",
      "epoch 4040 train loss 110312.32 test loss 158423.23\n",
      "epoch 4050 train loss 109944.37 test loss 158253.5\n",
      "epoch 4060 train loss 109591.73 test loss 158097.92\n",
      "epoch 4070 train loss 109248.336 test loss 157938.94\n",
      "epoch 4080 train loss 108912.58 test loss 157766.03\n",
      "epoch 4090 train loss 108583.56 test loss 157595.19\n",
      "epoch 4100 train loss 108267.695 test loss 157426.22\n",
      "epoch 4110 train loss 107976.086 test loss 157255.31\n",
      "epoch 4120 train loss 107693.19 test loss 157102.22\n",
      "epoch 4130 train loss 107409.555 test loss 156978.84\n",
      "epoch 4140 train loss 107115.234 test loss 156868.67\n",
      "epoch 4150 train loss 106823.67 test loss 156758.38\n",
      "epoch 4160 train loss 106541.82 test loss 156623.8\n",
      "epoch 4170 train loss 106265.04 test loss 156469.25\n",
      "epoch 4180 train loss 105992.45 test loss 156309.9\n",
      "epoch 4190 train loss 105732.6 test loss 156147.45\n",
      "epoch 4200 train loss 105487.76 test loss 155980.44\n",
      "epoch 4210 train loss 105254.86 test loss 155815.06\n",
      "epoch 4220 train loss 105023.04 test loss 155658.81\n",
      "epoch 4230 train loss 104787.38 test loss 155517.75\n",
      "epoch 4240 train loss 104553.07 test loss 155388.97\n",
      "epoch 4250 train loss 104320.09 test loss 155267.89\n",
      "epoch 4260 train loss 104089.46 test loss 155152.67\n",
      "epoch 4270 train loss 103872.35 test loss 155038.88\n",
      "epoch 4280 train loss 103660.125 test loss 154928.97\n",
      "epoch 4290 train loss 103451.125 test loss 154826.66\n",
      "epoch 4300 train loss 103244.77 test loss 154732.62\n",
      "epoch 4310 train loss 103040.77 test loss 154646.06\n",
      "epoch 4320 train loss 102838.95 test loss 154565.97\n",
      "epoch 4330 train loss 102639.28 test loss 154490.27\n",
      "epoch 4340 train loss 102441.65 test loss 154418.53\n",
      "epoch 4350 train loss 102246.055 test loss 154350.27\n",
      "epoch 4360 train loss 102052.42 test loss 154284.44\n",
      "epoch 4370 train loss 101860.73 test loss 154221.22\n",
      "epoch 4380 train loss 101670.945 test loss 154160.19\n",
      "epoch 4390 train loss 101483.016 test loss 154101.34\n",
      "epoch 4400 train loss 101297.23 test loss 154046.12\n",
      "epoch 4410 train loss 101113.42 test loss 153996.36\n",
      "epoch 4420 train loss 100931.41 test loss 153948.12\n",
      "epoch 4430 train loss 100751.11 test loss 153900.34\n",
      "epoch 4440 train loss 100572.664 test loss 153852.6\n",
      "epoch 4450 train loss 100396.27 test loss 153806.73\n",
      "epoch 4460 train loss 100221.58 test loss 153762.23\n",
      "epoch 4470 train loss 100048.55 test loss 153719.4\n",
      "epoch 4480 train loss 99877.09 test loss 153678.11\n",
      "epoch 4490 train loss 99707.234 test loss 153638.3\n",
      "epoch 4500 train loss 99539.11 test loss 153599.9\n",
      "epoch 4510 train loss 99377.23 test loss 153565.86\n",
      "epoch 4520 train loss 99217.875 test loss 153535.62\n",
      "epoch 4530 train loss 99060.19 test loss 153503.78\n",
      "epoch 4540 train loss 98903.945 test loss 153473.56\n",
      "epoch 4550 train loss 98748.98 test loss 153444.23\n",
      "epoch 4560 train loss 98595.26 test loss 153416.0\n",
      "epoch 4570 train loss 98442.77 test loss 153388.22\n",
      "epoch 4580 train loss 98291.484 test loss 153360.97\n",
      "epoch 4590 train loss 98141.1 test loss 153332.55\n",
      "epoch 4600 train loss 97991.49 test loss 153300.39\n",
      "epoch 4610 train loss 97842.95 test loss 153267.75\n",
      "epoch 4620 train loss 97695.56 test loss 153235.77\n",
      "epoch 4630 train loss 97549.32 test loss 153204.67\n",
      "epoch 4640 train loss 97404.18 test loss 153174.52\n",
      "epoch 4650 train loss 97260.18 test loss 153145.36\n",
      "epoch 4660 train loss 97117.29 test loss 153117.25\n",
      "epoch 4670 train loss 96975.516 test loss 153090.05\n",
      "epoch 4680 train loss 96834.805 test loss 153063.78\n",
      "epoch 4690 train loss 96695.19 test loss 153038.47\n",
      "epoch 4700 train loss 96556.65 test loss 153014.03\n",
      "epoch 4710 train loss 96419.23 test loss 152988.34\n",
      "epoch 4720 train loss 96282.836 test loss 152963.0\n",
      "epoch 4730 train loss 96147.52 test loss 152940.55\n",
      "epoch 4740 train loss 96013.26 test loss 152919.88\n",
      "epoch 4750 train loss 95880.01 test loss 152900.28\n",
      "epoch 4760 train loss 95747.78 test loss 152881.72\n",
      "epoch 4770 train loss 95616.56 test loss 152863.92\n",
      "epoch 4780 train loss 95486.32 test loss 152846.95\n",
      "epoch 4790 train loss 95357.64 test loss 152831.33\n",
      "epoch 4800 train loss 95232.89 test loss 152846.9\n",
      "epoch 4810 train loss 95109.81 test loss 152876.28\n",
      "epoch 4820 train loss 94988.01 test loss 152900.28\n",
      "epoch 4830 train loss 94867.37 test loss 152914.75\n",
      "epoch 4840 train loss 94747.695 test loss 152921.39\n",
      "epoch 4850 train loss 94628.984 test loss 152922.88\n",
      "epoch 4860 train loss 94511.25 test loss 152921.67\n",
      "epoch 4870 train loss 94394.47 test loss 152919.38\n",
      "epoch 4880 train loss 94278.62 test loss 152916.75\n",
      "epoch 4890 train loss 94163.67 test loss 152914.06\n",
      "epoch 4900 train loss 94049.6 test loss 152911.38\n",
      "epoch 4910 train loss 93936.44 test loss 152908.7\n",
      "epoch 4920 train loss 93824.15 test loss 152905.9\n",
      "epoch 4930 train loss 93712.72 test loss 152903.17\n",
      "epoch 4940 train loss 93602.15 test loss 152900.34\n",
      "epoch 4950 train loss 93492.4 test loss 152897.56\n",
      "epoch 4960 train loss 93383.61 test loss 152894.66\n",
      "epoch 4970 train loss 93276.2 test loss 152889.53\n",
      "epoch 4980 train loss 93169.695 test loss 152884.6\n",
      "epoch 4990 train loss 93064.01 test loss 152881.72\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_errs = []\n",
    "test_errs = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(5000):\n",
    "        # training is here\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        # Calculate train loss\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        train_errs.append(train_loss)\n",
    "        \n",
    "        # Calculate test loss\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        test_errs.append(test_loss)\n",
    "        \n",
    "        # Print losses\n",
    "        if epoch %10==0:\n",
    "            print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)\n",
    "    pred = sess.run(output, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505277889531616"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate r^2\n",
    "metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12c18f5f8>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c18fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(train_errs, label='Train loss')\n",
    "plt.plot(test_errs, label=\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#---------------        Mean Squared Error     ------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152880.94452557064"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987.3329"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.std(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011.338287535846"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:2522: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:2451: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:2451: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(y_test, pred)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12e91de10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12e990b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.scatter(y_test, pred, alpha=.9, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
