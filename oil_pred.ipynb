{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      ANN MODEL FOR PREDICTION OF BUBBLE POINT\n",
    "#                          PRESSURE OF CRUDE OILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL LIBRARIES USED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------EXCEL DATA---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Simulation Data (1).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOR</th>\n",
       "      <th>Oil gravity</th>\n",
       "      <th>Gas gravity</th>\n",
       "      <th>T</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1507</td>\n",
       "      <td>0.951</td>\n",
       "      <td>39.3</td>\n",
       "      <td>225</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898</td>\n",
       "      <td>0.802</td>\n",
       "      <td>32.7</td>\n",
       "      <td>175</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>1.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>898</td>\n",
       "      <td>0.802</td>\n",
       "      <td>32.7</td>\n",
       "      <td>150</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>1.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.930</td>\n",
       "      <td>42.8</td>\n",
       "      <td>235</td>\n",
       "      <td>3405.0</td>\n",
       "      <td>1.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825</td>\n",
       "      <td>0.779</td>\n",
       "      <td>34.2</td>\n",
       "      <td>185</td>\n",
       "      <td>3354.0</td>\n",
       "      <td>1.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GOR  Oil gravity  Gas gravity    T      Pb    Bob\n",
       "0  1507        0.951         39.3  225  3573.0  1.875\n",
       "1   898        0.802         32.7  175  3571.0  1.471\n",
       "2   898        0.802         32.7  150  3426.0  1.451\n",
       "3  1579        0.930         42.8  235  3405.0  1.997\n",
       "4   825        0.779         34.2  185  3354.0  1.431"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------   SPLIT DATA FOR TRAIN AND TEST ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/enkhbat/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(data[['GOR', 'Oil gravity', 'Gas gravity', 'T']], data.Pb)\n",
    "ss=StandardScaler()\n",
    "X_train=ss.fit_transform(X_train)\n",
    "X_test=ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape no matter how many col it became 1\n",
    "y_train=y_train.values.reshape(-1,1)\n",
    "y_test=y_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------- Building NN model ------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32, shape=(None,4), name='X' )\n",
    "y=tf.placeholder(dtype=tf.float32, shape=(None,1), name='y' )\n",
    "\n",
    "# setup hidden layer. data, neuron act func name\n",
    "h1=tf.layers.dense(X,8, activation=tf.nn.relu, name='hidden1')\n",
    "\n",
    "# output layer\n",
    "\n",
    "output=tf.layers.dense(h1, 1, activation=None, name='output')\n",
    "\n",
    "loss=tf.losses.mean_squared_error(y, output)\n",
    "optimizer=tf.train.AdamOptimizer(.01)\n",
    "\n",
    "training_run=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- ITERATED 50 EPOCH ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 4175242.8 test loss 5027713.0\n",
      "epoch 1 train loss 4174960.5 test loss 5027440.0\n",
      "epoch 2 train loss 4174674.5 test loss 5027165.0\n",
      "epoch 3 train loss 4174383.0 test loss 5026887.0\n",
      "epoch 4 train loss 4174085.5 test loss 5026606.5\n",
      "epoch 5 train loss 4173784.8 test loss 5026321.0\n",
      "epoch 6 train loss 4173478.2 test loss 5026031.5\n",
      "epoch 7 train loss 4173168.8 test loss 5025736.0\n",
      "epoch 8 train loss 4172853.8 test loss 5025436.0\n",
      "epoch 9 train loss 4172534.8 test loss 5025129.0\n",
      "epoch 10 train loss 4172211.5 test loss 5024818.0\n",
      "epoch 11 train loss 4171884.2 test loss 5024499.0\n",
      "epoch 12 train loss 4171552.2 test loss 5024173.0\n",
      "epoch 13 train loss 4171212.5 test loss 5023840.0\n",
      "epoch 14 train loss 4170868.8 test loss 5023498.0\n",
      "epoch 15 train loss 4170517.2 test loss 5023148.5\n",
      "epoch 16 train loss 4170159.8 test loss 5022790.0\n",
      "epoch 17 train loss 4169795.0 test loss 5022422.5\n",
      "epoch 18 train loss 4169424.5 test loss 5022046.5\n",
      "epoch 19 train loss 4169045.2 test loss 5021660.0\n",
      "epoch 20 train loss 4168659.0 test loss 5021266.0\n",
      "epoch 21 train loss 4168265.5 test loss 5020863.0\n",
      "epoch 22 train loss 4167862.5 test loss 5020450.0\n",
      "epoch 23 train loss 4167452.0 test loss 5020028.0\n",
      "epoch 24 train loss 4167031.2 test loss 5019596.0\n",
      "epoch 25 train loss 4166600.8 test loss 5019153.0\n",
      "epoch 26 train loss 4166160.5 test loss 5018699.5\n",
      "epoch 27 train loss 4165710.0 test loss 5018236.0\n",
      "epoch 28 train loss 4165249.8 test loss 5017761.5\n",
      "epoch 29 train loss 4164778.8 test loss 5017279.0\n",
      "epoch 30 train loss 4164297.8 test loss 5016785.5\n",
      "epoch 31 train loss 4163807.2 test loss 5016279.0\n",
      "epoch 32 train loss 4163304.8 test loss 5015759.0\n",
      "epoch 33 train loss 4162791.0 test loss 5015227.0\n",
      "epoch 34 train loss 4162264.0 test loss 5014680.5\n",
      "epoch 35 train loss 4161724.2 test loss 5014121.0\n",
      "epoch 36 train loss 4161172.2 test loss 5013548.0\n",
      "epoch 37 train loss 4160607.5 test loss 5012960.0\n",
      "epoch 38 train loss 4160030.2 test loss 5012358.0\n",
      "epoch 39 train loss 4159439.5 test loss 5011739.5\n",
      "epoch 40 train loss 4158836.2 test loss 5011105.0\n",
      "epoch 41 train loss 4158218.2 test loss 5010455.0\n",
      "epoch 42 train loss 4157587.0 test loss 5009790.0\n",
      "epoch 43 train loss 4156940.5 test loss 5009110.0\n",
      "epoch 44 train loss 4156275.2 test loss 5008413.5\n",
      "epoch 45 train loss 4155596.5 test loss 5007702.0\n",
      "epoch 46 train loss 4154903.2 test loss 5006974.5\n",
      "epoch 47 train loss 4154195.8 test loss 5006232.0\n",
      "epoch 48 train loss 4153472.8 test loss 5005472.0\n",
      "epoch 49 train loss 4152732.8 test loss 5004695.5\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------5000 epoch iteration ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 4174372.0 test loss 5025887.0\n",
      "epoch 10 train loss 4171230.8 test loss 5022272.5\n",
      "epoch 20 train loss 4167722.8 test loss 5018158.0\n",
      "epoch 30 train loss 4163442.8 test loss 5013187.0\n",
      "epoch 40 train loss 4158146.0 test loss 5006977.0\n",
      "epoch 50 train loss 4151486.8 test loss 4999149.5\n",
      "epoch 60 train loss 4143273.0 test loss 4989478.0\n",
      "epoch 70 train loss 4133200.5 test loss 4977611.0\n",
      "epoch 80 train loss 4120810.0 test loss 4962980.5\n",
      "epoch 90 train loss 4105829.2 test loss 4945167.0\n",
      "epoch 100 train loss 4088048.8 test loss 4924259.5\n",
      "epoch 110 train loss 4067511.5 test loss 4900290.0\n",
      "epoch 120 train loss 4044204.0 test loss 4873257.0\n",
      "epoch 130 train loss 4017923.5 test loss 4843012.0\n",
      "epoch 140 train loss 3988549.0 test loss 4809207.0\n",
      "epoch 150 train loss 3955984.0 test loss 4771592.0\n",
      "epoch 160 train loss 3920106.8 test loss 4730152.0\n",
      "epoch 170 train loss 3881258.0 test loss 4685138.0\n",
      "epoch 180 train loss 3839652.2 test loss 4636684.5\n",
      "epoch 190 train loss 3795336.8 test loss 4584866.0\n",
      "epoch 200 train loss 3748472.5 test loss 4530086.0\n",
      "epoch 210 train loss 3699272.5 test loss 4472560.0\n",
      "epoch 220 train loss 3647904.5 test loss 4412517.0\n",
      "epoch 230 train loss 3594468.0 test loss 4350144.0\n",
      "epoch 240 train loss 3539223.0 test loss 4285596.5\n",
      "epoch 250 train loss 3482359.8 test loss 4219069.0\n",
      "epoch 260 train loss 3424051.5 test loss 4150734.5\n",
      "epoch 270 train loss 3364481.2 test loss 4080753.5\n",
      "epoch 280 train loss 3303813.8 test loss 4009284.5\n",
      "epoch 290 train loss 3242173.8 test loss 3936443.0\n",
      "epoch 300 train loss 3179713.2 test loss 3862374.0\n",
      "epoch 310 train loss 3116593.8 test loss 3787267.5\n",
      "epoch 320 train loss 3052957.5 test loss 3711254.5\n",
      "epoch 330 train loss 2988921.2 test loss 3634483.5\n",
      "epoch 340 train loss 2924602.5 test loss 3557091.0\n",
      "epoch 350 train loss 2860128.2 test loss 3479220.0\n",
      "epoch 360 train loss 2795600.2 test loss 3400994.0\n",
      "epoch 370 train loss 2731110.0 test loss 3322540.8\n",
      "epoch 380 train loss 2666743.0 test loss 3244006.0\n",
      "epoch 390 train loss 2602595.0 test loss 3165478.5\n",
      "epoch 400 train loss 2538749.0 test loss 3087045.5\n",
      "epoch 410 train loss 2475273.0 test loss 3008799.0\n",
      "epoch 420 train loss 2412217.0 test loss 2930835.2\n",
      "epoch 430 train loss 2349640.2 test loss 2853225.5\n",
      "epoch 440 train loss 2287590.0 test loss 2776037.5\n",
      "epoch 450 train loss 2226138.2 test loss 2699365.0\n",
      "epoch 460 train loss 2165309.8 test loss 2623260.5\n",
      "epoch 470 train loss 2105170.0 test loss 2547811.5\n",
      "epoch 480 train loss 2045776.9 test loss 2473107.0\n",
      "epoch 490 train loss 1987177.1 test loss 2399234.0\n",
      "epoch 500 train loss 1929399.9 test loss 2326222.8\n",
      "epoch 510 train loss 1872474.1 test loss 2254077.5\n",
      "epoch 520 train loss 1816438.4 test loss 2182895.0\n",
      "epoch 530 train loss 1761340.2 test loss 2112734.0\n",
      "epoch 540 train loss 1707204.2 test loss 2043616.5\n",
      "epoch 550 train loss 1653993.9 test loss 1975538.8\n",
      "epoch 560 train loss 1601752.8 test loss 1908624.2\n",
      "epoch 570 train loss 1550423.8 test loss 1842878.8\n",
      "epoch 580 train loss 1500010.0 test loss 1778326.8\n",
      "epoch 590 train loss 1450593.5 test loss 1715051.4\n",
      "epoch 600 train loss 1402280.4 test loss 1653178.1\n",
      "epoch 610 train loss 1355079.0 test loss 1592767.5\n",
      "epoch 620 train loss 1308942.2 test loss 1533754.5\n",
      "epoch 630 train loss 1263944.6 test loss 1476238.2\n",
      "epoch 640 train loss 1220183.5 test loss 1420340.5\n",
      "epoch 650 train loss 1177672.6 test loss 1366120.2\n",
      "epoch 660 train loss 1136374.9 test loss 1313529.8\n",
      "epoch 670 train loss 1096372.0 test loss 1262630.0\n",
      "epoch 680 train loss 1057705.4 test loss 1213507.4\n",
      "epoch 690 train loss 1020324.06 test loss 1166078.1\n",
      "epoch 700 train loss 984211.6 test loss 1120341.4\n",
      "epoch 710 train loss 949400.2 test loss 1076325.4\n",
      "epoch 720 train loss 915851.0 test loss 1034064.25\n",
      "epoch 730 train loss 883629.7 test loss 993570.8\n",
      "epoch 740 train loss 852742.7 test loss 954850.9\n",
      "epoch 750 train loss 823180.2 test loss 917889.9\n",
      "epoch 760 train loss 794917.94 test loss 882663.25\n",
      "epoch 770 train loss 767922.7 test loss 849125.5\n",
      "epoch 780 train loss 742186.75 test loss 817192.94\n",
      "epoch 790 train loss 717678.25 test loss 786893.1\n",
      "epoch 800 train loss 694351.1 test loss 758174.2\n",
      "epoch 810 train loss 672175.7 test loss 730991.7\n",
      "epoch 820 train loss 651103.6 test loss 705297.56\n",
      "epoch 830 train loss 631112.06 test loss 681044.56\n",
      "epoch 840 train loss 612174.4 test loss 658191.0\n",
      "epoch 850 train loss 594281.1 test loss 636698.25\n",
      "epoch 860 train loss 577400.56 test loss 616517.94\n",
      "epoch 870 train loss 561488.3 test loss 597595.7\n",
      "epoch 880 train loss 546505.4 test loss 579873.9\n",
      "epoch 890 train loss 532417.5 test loss 563249.2\n",
      "epoch 900 train loss 519185.8 test loss 547633.94\n",
      "epoch 910 train loss 506765.28 test loss 533053.75\n",
      "epoch 920 train loss 495101.72 test loss 519431.2\n",
      "epoch 930 train loss 484139.12 test loss 506711.2\n",
      "epoch 940 train loss 473847.53 test loss 494841.62\n",
      "epoch 950 train loss 464211.47 test loss 483780.34\n",
      "epoch 960 train loss 455198.9 test loss 473475.7\n",
      "epoch 970 train loss 446771.56 test loss 463872.84\n",
      "epoch 980 train loss 438889.9 test loss 454928.88\n",
      "epoch 990 train loss 431503.53 test loss 446590.56\n",
      "epoch 1000 train loss 424587.25 test loss 438803.9\n",
      "epoch 1010 train loss 418118.97 test loss 431543.16\n",
      "epoch 1020 train loss 412070.53 test loss 424757.2\n",
      "epoch 1030 train loss 406405.16 test loss 418408.3\n",
      "epoch 1040 train loss 401084.94 test loss 412466.7\n",
      "epoch 1050 train loss 396093.25 test loss 406901.25\n",
      "epoch 1060 train loss 391411.5 test loss 401683.56\n",
      "epoch 1070 train loss 387010.88 test loss 396782.12\n",
      "epoch 1080 train loss 382855.75 test loss 392146.2\n",
      "epoch 1090 train loss 378925.16 test loss 387759.44\n",
      "epoch 1100 train loss 375209.53 test loss 383598.4\n",
      "epoch 1110 train loss 371690.47 test loss 379643.0\n",
      "epoch 1120 train loss 368353.53 test loss 375885.7\n",
      "epoch 1130 train loss 365074.03 test loss 372244.62\n",
      "epoch 1140 train loss 361877.6 test loss 368689.62\n",
      "epoch 1150 train loss 358803.9 test loss 365252.38\n",
      "epoch 1160 train loss 355852.2 test loss 361934.3\n",
      "epoch 1170 train loss 353022.47 test loss 358727.5\n",
      "epoch 1180 train loss 350306.53 test loss 355623.5\n",
      "epoch 1190 train loss 347693.84 test loss 352608.2\n",
      "epoch 1200 train loss 345180.88 test loss 349670.75\n",
      "epoch 1210 train loss 342746.97 test loss 346784.88\n",
      "epoch 1220 train loss 340390.6 test loss 343968.88\n",
      "epoch 1230 train loss 338114.62 test loss 341223.12\n",
      "epoch 1240 train loss 335912.28 test loss 338544.94\n",
      "epoch 1250 train loss 333739.53 test loss 335917.34\n",
      "epoch 1260 train loss 331614.94 test loss 333335.0\n",
      "epoch 1270 train loss 329544.53 test loss 330803.06\n",
      "epoch 1280 train loss 327509.03 test loss 328319.0\n",
      "epoch 1290 train loss 325519.62 test loss 325878.8\n",
      "epoch 1300 train loss 323573.28 test loss 323481.4\n",
      "epoch 1310 train loss 321652.44 test loss 321103.8\n",
      "epoch 1320 train loss 319772.6 test loss 318753.06\n",
      "epoch 1330 train loss 317935.3 test loss 316439.12\n",
      "epoch 1340 train loss 316134.53 test loss 314161.06\n",
      "epoch 1350 train loss 314367.38 test loss 311907.28\n",
      "epoch 1360 train loss 312627.8 test loss 309674.62\n",
      "epoch 1370 train loss 310922.53 test loss 307475.12\n",
      "epoch 1380 train loss 309251.97 test loss 305304.56\n",
      "epoch 1390 train loss 307612.72 test loss 303160.97\n",
      "epoch 1400 train loss 305999.9 test loss 301048.06\n",
      "epoch 1410 train loss 304406.25 test loss 298961.3\n",
      "epoch 1420 train loss 302824.38 test loss 296896.1\n",
      "epoch 1430 train loss 301266.4 test loss 294854.62\n",
      "epoch 1440 train loss 299736.8 test loss 292842.66\n",
      "epoch 1450 train loss 298232.6 test loss 290845.75\n",
      "epoch 1460 train loss 296731.4 test loss 288865.44\n",
      "epoch 1470 train loss 295248.94 test loss 286911.0\n",
      "epoch 1480 train loss 293791.66 test loss 284985.88\n",
      "epoch 1490 train loss 292355.72 test loss 283089.78\n",
      "epoch 1500 train loss 290942.6 test loss 281117.2\n",
      "epoch 1510 train loss 289541.47 test loss 279056.53\n",
      "epoch 1520 train loss 288154.78 test loss 276994.03\n",
      "epoch 1530 train loss 286772.5 test loss 274844.25\n",
      "epoch 1540 train loss 285410.34 test loss 272679.62\n",
      "epoch 1550 train loss 284072.56 test loss 270544.75\n",
      "epoch 1560 train loss 282758.97 test loss 268441.38\n",
      "epoch 1570 train loss 281462.12 test loss 266343.75\n",
      "epoch 1580 train loss 280151.06 test loss 264243.62\n",
      "epoch 1590 train loss 278829.8 test loss 262125.28\n",
      "epoch 1600 train loss 277465.25 test loss 259982.61\n",
      "epoch 1610 train loss 276059.72 test loss 257776.19\n",
      "epoch 1620 train loss 274607.38 test loss 255549.56\n",
      "epoch 1630 train loss 273100.25 test loss 253279.39\n",
      "epoch 1640 train loss 271564.8 test loss 251001.14\n",
      "epoch 1650 train loss 270021.34 test loss 248734.45\n",
      "epoch 1660 train loss 268485.75 test loss 246494.95\n",
      "epoch 1670 train loss 266872.4 test loss 244229.11\n",
      "epoch 1680 train loss 265122.62 test loss 241855.23\n",
      "epoch 1690 train loss 263231.06 test loss 239414.61\n",
      "epoch 1700 train loss 261247.88 test loss 236910.06\n",
      "epoch 1710 train loss 259216.08 test loss 234399.86\n",
      "epoch 1720 train loss 257054.45 test loss 231832.23\n",
      "epoch 1730 train loss 254812.61 test loss 229237.77\n",
      "epoch 1740 train loss 252517.1 test loss 226640.69\n",
      "epoch 1750 train loss 250246.11 test loss 224088.23\n",
      "epoch 1760 train loss 247918.88 test loss 221573.97\n",
      "epoch 1770 train loss 245552.25 test loss 219075.25\n",
      "epoch 1780 train loss 243107.22 test loss 216595.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1790 train loss 240671.69 test loss 214146.95\n",
      "epoch 1800 train loss 238266.75 test loss 211751.83\n",
      "epoch 1810 train loss 235801.08 test loss 209382.48\n",
      "epoch 1820 train loss 233236.34 test loss 207037.95\n",
      "epoch 1830 train loss 230549.84 test loss 204697.9\n",
      "epoch 1840 train loss 227765.61 test loss 202366.62\n",
      "epoch 1850 train loss 224858.14 test loss 200015.62\n",
      "epoch 1860 train loss 221904.52 test loss 197657.3\n",
      "epoch 1870 train loss 218897.12 test loss 195333.69\n",
      "epoch 1880 train loss 215780.22 test loss 193043.98\n",
      "epoch 1890 train loss 212634.25 test loss 190772.97\n",
      "epoch 1900 train loss 209558.52 test loss 188553.16\n",
      "epoch 1910 train loss 206495.39 test loss 186390.66\n",
      "epoch 1920 train loss 203448.02 test loss 184279.44\n",
      "epoch 1930 train loss 200425.73 test loss 182218.02\n",
      "epoch 1940 train loss 197494.98 test loss 180221.45\n",
      "epoch 1950 train loss 194659.66 test loss 178298.48\n",
      "epoch 1960 train loss 191904.47 test loss 176420.73\n",
      "epoch 1970 train loss 189226.69 test loss 174574.03\n",
      "epoch 1980 train loss 186621.84 test loss 172705.5\n",
      "epoch 1990 train loss 184120.19 test loss 170878.88\n",
      "epoch 2000 train loss 181702.28 test loss 169137.38\n",
      "epoch 2010 train loss 179337.45 test loss 167456.38\n",
      "epoch 2020 train loss 177021.78 test loss 165821.16\n",
      "epoch 2030 train loss 174780.6 test loss 164251.12\n",
      "epoch 2040 train loss 172615.45 test loss 162729.92\n",
      "epoch 2050 train loss 170529.23 test loss 161262.9\n",
      "epoch 2060 train loss 168512.38 test loss 159814.97\n",
      "epoch 2070 train loss 166555.5 test loss 158425.55\n",
      "epoch 2080 train loss 164661.42 test loss 156994.97\n",
      "epoch 2090 train loss 162824.48 test loss 155629.72\n",
      "epoch 2100 train loss 161049.4 test loss 154331.22\n",
      "epoch 2110 train loss 159334.62 test loss 153098.38\n",
      "epoch 2120 train loss 157636.98 test loss 151915.16\n",
      "epoch 2130 train loss 155932.61 test loss 150765.44\n",
      "epoch 2140 train loss 154236.8 test loss 149636.47\n",
      "epoch 2150 train loss 152568.33 test loss 148567.0\n",
      "epoch 2160 train loss 150948.14 test loss 147558.9\n",
      "epoch 2170 train loss 149384.88 test loss 146614.61\n",
      "epoch 2180 train loss 147869.39 test loss 145730.78\n",
      "epoch 2190 train loss 146388.5 test loss 144897.47\n",
      "epoch 2200 train loss 144950.23 test loss 144113.3\n",
      "epoch 2210 train loss 143558.2 test loss 143377.38\n",
      "epoch 2220 train loss 142214.8 test loss 142691.06\n",
      "epoch 2230 train loss 140919.73 test loss 142053.4\n",
      "epoch 2240 train loss 139671.97 test loss 141453.64\n",
      "epoch 2250 train loss 138460.58 test loss 140888.6\n",
      "epoch 2260 train loss 137281.62 test loss 140351.12\n",
      "epoch 2270 train loss 136114.05 test loss 139844.12\n",
      "epoch 2280 train loss 134966.75 test loss 139366.16\n",
      "epoch 2290 train loss 133854.94 test loss 138932.6\n",
      "epoch 2300 train loss 132782.44 test loss 138540.11\n",
      "epoch 2310 train loss 131748.97 test loss 138186.66\n",
      "epoch 2320 train loss 130753.234 test loss 137868.4\n",
      "epoch 2330 train loss 129794.484 test loss 137581.22\n",
      "epoch 2340 train loss 128871.516 test loss 137321.78\n",
      "epoch 2350 train loss 127985.38 test loss 137081.08\n",
      "epoch 2360 train loss 127128.51 test loss 136857.84\n",
      "epoch 2370 train loss 126298.66 test loss 136656.78\n",
      "epoch 2380 train loss 125498.766 test loss 136480.19\n",
      "epoch 2390 train loss 124728.484 test loss 136328.39\n",
      "epoch 2400 train loss 123986.72 test loss 136199.8\n",
      "epoch 2410 train loss 123272.3 test loss 136092.4\n",
      "epoch 2420 train loss 122583.96 test loss 136001.48\n",
      "epoch 2430 train loss 121917.34 test loss 135927.02\n",
      "epoch 2440 train loss 121261.16 test loss 135868.05\n",
      "epoch 2450 train loss 120611.31 test loss 135825.4\n",
      "epoch 2460 train loss 119961.64 test loss 135797.19\n",
      "epoch 2470 train loss 119321.71 test loss 135780.55\n",
      "epoch 2480 train loss 118672.18 test loss 135774.78\n",
      "epoch 2490 train loss 118010.33 test loss 135781.0\n",
      "epoch 2500 train loss 117356.44 test loss 135804.27\n",
      "epoch 2510 train loss 116723.09 test loss 135845.4\n",
      "epoch 2520 train loss 116112.83 test loss 135901.48\n",
      "epoch 2530 train loss 115524.984 test loss 135968.78\n",
      "epoch 2540 train loss 114952.53 test loss 136041.8\n",
      "epoch 2550 train loss 114382.33 test loss 136117.52\n",
      "epoch 2560 train loss 113820.6 test loss 136199.72\n",
      "epoch 2570 train loss 113269.52 test loss 136295.4\n",
      "epoch 2580 train loss 112737.1 test loss 136405.55\n",
      "epoch 2590 train loss 112225.53 test loss 136526.31\n",
      "epoch 2600 train loss 111735.555 test loss 136654.0\n",
      "epoch 2610 train loss 111265.17 test loss 136785.47\n",
      "epoch 2620 train loss 110815.234 test loss 136918.61\n",
      "epoch 2630 train loss 110385.31 test loss 137053.95\n",
      "epoch 2640 train loss 109972.516 test loss 137171.25\n",
      "epoch 2650 train loss 109562.83 test loss 137261.62\n",
      "epoch 2660 train loss 109163.04 test loss 137320.6\n",
      "epoch 2670 train loss 108778.29 test loss 137384.62\n",
      "epoch 2680 train loss 108408.16 test loss 137456.9\n",
      "epoch 2690 train loss 108051.22 test loss 137534.6\n",
      "epoch 2700 train loss 107710.45 test loss 137615.06\n",
      "epoch 2710 train loss 107384.91 test loss 137698.72\n",
      "epoch 2720 train loss 107073.875 test loss 137783.69\n",
      "epoch 2730 train loss 106776.58 test loss 137871.16\n",
      "epoch 2740 train loss 106492.33 test loss 137959.81\n",
      "epoch 2750 train loss 106220.0 test loss 138047.53\n",
      "epoch 2760 train loss 105959.34 test loss 138135.44\n",
      "epoch 2770 train loss 105709.86 test loss 138224.0\n",
      "epoch 2780 train loss 105472.15 test loss 138312.6\n",
      "epoch 2790 train loss 105245.22 test loss 138399.42\n",
      "epoch 2800 train loss 105032.19 test loss 138479.19\n",
      "epoch 2810 train loss 104829.53 test loss 138549.1\n",
      "epoch 2820 train loss 104634.2 test loss 138615.81\n",
      "epoch 2830 train loss 104446.86 test loss 138682.12\n",
      "epoch 2840 train loss 104267.22 test loss 138748.45\n",
      "epoch 2850 train loss 104095.164 test loss 138813.22\n",
      "epoch 2860 train loss 103931.45 test loss 138876.86\n",
      "epoch 2870 train loss 103774.766 test loss 138939.53\n",
      "epoch 2880 train loss 103625.05 test loss 139000.2\n",
      "epoch 2890 train loss 103481.35 test loss 139058.3\n",
      "epoch 2900 train loss 103342.99 test loss 139114.97\n",
      "epoch 2910 train loss 103209.83 test loss 139171.22\n",
      "epoch 2920 train loss 103081.35 test loss 139226.44\n",
      "epoch 2930 train loss 102957.375 test loss 139279.6\n",
      "epoch 2940 train loss 102839.31 test loss 139331.53\n",
      "epoch 2950 train loss 102725.62 test loss 139382.53\n",
      "epoch 2960 train loss 102615.81 test loss 139431.77\n",
      "epoch 2970 train loss 102509.5 test loss 139478.36\n",
      "epoch 2980 train loss 102406.35 test loss 139521.84\n",
      "epoch 2990 train loss 102306.27 test loss 139562.5\n",
      "epoch 3000 train loss 102203.6 test loss 139600.84\n",
      "epoch 3010 train loss 102100.875 test loss 139636.47\n",
      "epoch 3020 train loss 102000.01 test loss 139664.73\n",
      "epoch 3030 train loss 101896.38 test loss 139686.72\n",
      "epoch 3040 train loss 101793.734 test loss 139706.1\n",
      "epoch 3050 train loss 101694.17 test loss 139722.28\n",
      "epoch 3060 train loss 101598.06 test loss 139734.12\n",
      "epoch 3070 train loss 101504.21 test loss 139743.6\n",
      "epoch 3080 train loss 101412.305 test loss 139751.17\n",
      "epoch 3090 train loss 101322.17 test loss 139756.31\n",
      "epoch 3100 train loss 101233.766 test loss 139759.31\n",
      "epoch 3110 train loss 101147.08 test loss 139760.44\n",
      "epoch 3120 train loss 101062.01 test loss 139756.23\n",
      "epoch 3130 train loss 100978.44 test loss 139743.73\n",
      "epoch 3140 train loss 100896.11 test loss 139729.64\n",
      "epoch 3150 train loss 100811.84 test loss 139716.86\n",
      "epoch 3160 train loss 100727.88 test loss 139702.69\n",
      "epoch 3170 train loss 100644.984 test loss 139684.73\n",
      "epoch 3180 train loss 100563.28 test loss 139663.42\n",
      "epoch 3190 train loss 100483.28 test loss 139643.81\n",
      "epoch 3200 train loss 100404.48 test loss 139623.86\n",
      "epoch 3210 train loss 100326.6 test loss 139600.02\n",
      "epoch 3220 train loss 100249.5 test loss 139572.69\n",
      "epoch 3230 train loss 100173.7 test loss 139545.19\n",
      "epoch 3240 train loss 100098.945 test loss 139518.38\n",
      "epoch 3250 train loss 100025.04 test loss 139489.97\n",
      "epoch 3260 train loss 99952.14 test loss 139460.9\n",
      "epoch 3270 train loss 99880.22 test loss 139434.22\n",
      "epoch 3280 train loss 99809.016 test loss 139405.02\n",
      "epoch 3290 train loss 99738.47 test loss 139372.02\n",
      "epoch 3300 train loss 99668.56 test loss 139336.38\n",
      "epoch 3310 train loss 99599.125 test loss 139298.53\n",
      "epoch 3320 train loss 99530.234 test loss 139254.88\n",
      "epoch 3330 train loss 99461.94 test loss 139209.97\n",
      "epoch 3340 train loss 99394.18 test loss 139164.34\n",
      "epoch 3350 train loss 99326.96 test loss 139117.8\n",
      "epoch 3360 train loss 99260.336 test loss 139068.12\n",
      "epoch 3370 train loss 99194.234 test loss 139016.31\n",
      "epoch 3380 train loss 99128.68 test loss 138964.52\n",
      "epoch 3390 train loss 99063.766 test loss 138913.31\n",
      "epoch 3400 train loss 98999.555 test loss 138865.0\n",
      "epoch 3410 train loss 98935.875 test loss 138815.78\n",
      "epoch 3420 train loss 98872.66 test loss 138765.19\n",
      "epoch 3430 train loss 98809.914 test loss 138713.31\n",
      "epoch 3440 train loss 98747.586 test loss 138660.55\n",
      "epoch 3450 train loss 98685.65 test loss 138607.25\n",
      "epoch 3460 train loss 98624.16 test loss 138553.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3470 train loss 98563.055 test loss 138498.77\n",
      "epoch 3480 train loss 98502.336 test loss 138443.56\n",
      "epoch 3490 train loss 98442.04 test loss 138388.08\n",
      "epoch 3500 train loss 98382.09 test loss 138332.38\n",
      "epoch 3510 train loss 98322.53 test loss 138276.28\n",
      "epoch 3520 train loss 98263.32 test loss 138219.84\n",
      "epoch 3530 train loss 98204.47 test loss 138163.03\n",
      "epoch 3540 train loss 98145.96 test loss 138105.78\n",
      "epoch 3550 train loss 98087.8 test loss 138048.58\n",
      "epoch 3560 train loss 98029.94 test loss 137990.9\n",
      "epoch 3570 train loss 97972.44 test loss 137933.14\n",
      "epoch 3580 train loss 97915.25 test loss 137875.03\n",
      "epoch 3590 train loss 97858.38 test loss 137816.88\n",
      "epoch 3600 train loss 97801.945 test loss 137757.22\n",
      "epoch 3610 train loss 97746.03 test loss 137696.34\n",
      "epoch 3620 train loss 97690.04 test loss 137636.06\n",
      "epoch 3630 train loss 97633.74 test loss 137575.89\n",
      "epoch 3640 train loss 97577.65 test loss 137517.03\n",
      "epoch 3650 train loss 97521.83 test loss 137459.8\n",
      "epoch 3660 train loss 97466.37 test loss 137402.6\n",
      "epoch 3670 train loss 97411.234 test loss 137344.56\n",
      "epoch 3680 train loss 97356.48 test loss 137286.56\n",
      "epoch 3690 train loss 97302.0 test loss 137228.98\n",
      "epoch 3700 train loss 97248.266 test loss 137169.64\n",
      "epoch 3710 train loss 97195.01 test loss 137108.06\n",
      "epoch 3720 train loss 97142.14 test loss 137046.42\n",
      "epoch 3730 train loss 97089.63 test loss 136984.72\n",
      "epoch 3740 train loss 97037.766 test loss 136920.28\n",
      "epoch 3750 train loss 96986.14 test loss 136854.14\n",
      "epoch 3760 train loss 96934.81 test loss 136787.97\n",
      "epoch 3770 train loss 96883.62 test loss 136721.94\n",
      "epoch 3780 train loss 96832.63 test loss 136656.03\n",
      "epoch 3790 train loss 96781.52 test loss 136589.75\n",
      "epoch 3800 train loss 96729.86 test loss 136522.7\n",
      "epoch 3810 train loss 96678.09 test loss 136458.83\n",
      "epoch 3820 train loss 96625.375 test loss 136396.62\n",
      "epoch 3830 train loss 96572.48 test loss 136337.16\n",
      "epoch 3840 train loss 96519.94 test loss 136283.52\n",
      "epoch 3850 train loss 96467.734 test loss 136235.47\n",
      "epoch 3860 train loss 96415.914 test loss 136176.77\n",
      "epoch 3870 train loss 96364.32 test loss 136115.03\n",
      "epoch 3880 train loss 96313.02 test loss 136056.6\n",
      "epoch 3890 train loss 96261.93 test loss 135999.03\n",
      "epoch 3900 train loss 96210.95 test loss 135939.75\n",
      "epoch 3910 train loss 96159.96 test loss 135875.84\n",
      "epoch 3920 train loss 96109.04 test loss 135809.2\n",
      "epoch 3930 train loss 96057.88 test loss 135745.47\n",
      "epoch 3940 train loss 96006.68 test loss 135680.4\n",
      "epoch 3950 train loss 95955.67 test loss 135615.23\n",
      "epoch 3960 train loss 95904.9 test loss 135554.56\n",
      "epoch 3970 train loss 95854.24 test loss 135493.47\n",
      "epoch 3980 train loss 95803.734 test loss 135429.22\n",
      "epoch 3990 train loss 95753.34 test loss 135364.4\n",
      "epoch 4000 train loss 95703.13 test loss 135298.88\n",
      "epoch 4010 train loss 95653.02 test loss 135233.78\n",
      "epoch 4020 train loss 95603.05 test loss 135168.4\n",
      "epoch 4030 train loss 95553.19 test loss 135102.36\n",
      "epoch 4040 train loss 95502.66 test loss 135035.03\n",
      "epoch 4050 train loss 95451.83 test loss 134968.03\n",
      "epoch 4060 train loss 95401.27 test loss 134905.53\n",
      "epoch 4070 train loss 95349.76 test loss 134847.84\n",
      "epoch 4080 train loss 95296.68 test loss 134788.22\n",
      "epoch 4090 train loss 95243.31 test loss 134726.53\n",
      "epoch 4100 train loss 95189.94 test loss 134663.11\n",
      "epoch 4110 train loss 95136.664 test loss 134598.33\n",
      "epoch 4120 train loss 95083.52 test loss 134532.64\n",
      "epoch 4130 train loss 95030.516 test loss 134466.72\n",
      "epoch 4140 train loss 94977.79 test loss 134400.44\n",
      "epoch 4150 train loss 94925.35 test loss 134333.34\n",
      "epoch 4160 train loss 94873.1 test loss 134266.33\n",
      "epoch 4170 train loss 94821.664 test loss 134200.52\n",
      "epoch 4180 train loss 94770.96 test loss 134138.06\n",
      "epoch 4190 train loss 94718.98 test loss 134080.42\n",
      "epoch 4200 train loss 94666.16 test loss 134027.6\n",
      "epoch 4210 train loss 94613.22 test loss 133987.08\n",
      "epoch 4220 train loss 94560.234 test loss 133954.75\n",
      "epoch 4230 train loss 94507.68 test loss 133925.28\n",
      "epoch 4240 train loss 94455.53 test loss 133899.08\n",
      "epoch 4250 train loss 94403.64 test loss 133867.28\n",
      "epoch 4260 train loss 94352.32 test loss 133832.19\n",
      "epoch 4270 train loss 94301.29 test loss 133794.31\n",
      "epoch 4280 train loss 94250.48 test loss 133754.14\n",
      "epoch 4290 train loss 94199.91 test loss 133707.62\n",
      "epoch 4300 train loss 94149.49 test loss 133658.25\n",
      "epoch 4310 train loss 94099.31 test loss 133606.03\n",
      "epoch 4320 train loss 94049.44 test loss 133550.9\n",
      "epoch 4330 train loss 93999.766 test loss 133499.75\n",
      "epoch 4340 train loss 93950.29 test loss 133447.97\n",
      "epoch 4350 train loss 93901.055 test loss 133393.6\n",
      "epoch 4360 train loss 93851.98 test loss 133337.06\n",
      "epoch 4370 train loss 93802.87 test loss 133279.0\n",
      "epoch 4380 train loss 93753.89 test loss 133218.75\n",
      "epoch 4390 train loss 93705.09 test loss 133157.28\n",
      "epoch 4400 train loss 93656.516 test loss 133095.73\n",
      "epoch 4410 train loss 93608.125 test loss 133033.88\n",
      "epoch 4420 train loss 93559.96 test loss 132971.89\n",
      "epoch 4430 train loss 93512.125 test loss 132909.83\n",
      "epoch 4440 train loss 93464.59 test loss 132845.31\n",
      "epoch 4450 train loss 93417.25 test loss 132780.8\n",
      "epoch 4460 train loss 93370.125 test loss 132716.19\n",
      "epoch 4470 train loss 93323.19 test loss 132651.19\n",
      "epoch 4480 train loss 93276.47 test loss 132587.1\n",
      "epoch 4490 train loss 93230.03 test loss 132522.39\n",
      "epoch 4500 train loss 93183.82 test loss 132456.98\n",
      "epoch 4510 train loss 93139.5 test loss 132387.94\n",
      "epoch 4520 train loss 93095.55 test loss 132313.55\n",
      "epoch 4530 train loss 93051.91 test loss 132240.34\n",
      "epoch 4540 train loss 93008.58 test loss 132170.4\n",
      "epoch 4550 train loss 92965.484 test loss 132101.8\n",
      "epoch 4560 train loss 92922.06 test loss 132035.94\n",
      "epoch 4570 train loss 92877.766 test loss 131975.58\n",
      "epoch 4580 train loss 92833.44 test loss 131921.0\n",
      "epoch 4590 train loss 92789.336 test loss 131869.9\n",
      "epoch 4600 train loss 92745.49 test loss 131819.75\n",
      "epoch 4610 train loss 92701.945 test loss 131768.84\n",
      "epoch 4620 train loss 92658.68 test loss 131716.89\n",
      "epoch 4630 train loss 92615.305 test loss 131662.88\n",
      "epoch 4640 train loss 92571.86 test loss 131606.66\n",
      "epoch 4650 train loss 92528.55 test loss 131549.6\n",
      "epoch 4660 train loss 92485.81 test loss 131501.27\n",
      "epoch 4670 train loss 92443.26 test loss 131453.55\n",
      "epoch 4680 train loss 92400.9 test loss 131399.42\n",
      "epoch 4690 train loss 92358.766 test loss 131341.25\n",
      "epoch 4700 train loss 92315.19 test loss 131279.4\n",
      "epoch 4710 train loss 92270.38 test loss 131213.9\n",
      "epoch 4720 train loss 92225.53 test loss 131150.47\n",
      "epoch 4730 train loss 92180.75 test loss 131089.84\n",
      "epoch 4740 train loss 92135.76 test loss 131025.734\n",
      "epoch 4750 train loss 92090.805 test loss 130960.45\n",
      "epoch 4760 train loss 92046.01 test loss 130893.95\n",
      "epoch 4770 train loss 92001.45 test loss 130830.36\n",
      "epoch 4780 train loss 91957.09 test loss 130767.84\n",
      "epoch 4790 train loss 91913.195 test loss 130705.805\n",
      "epoch 4800 train loss 91870.055 test loss 130646.5\n",
      "epoch 4810 train loss 91827.195 test loss 130590.88\n",
      "epoch 4820 train loss 91784.62 test loss 130535.91\n",
      "epoch 4830 train loss 91742.26 test loss 130477.805\n",
      "epoch 4840 train loss 91700.14 test loss 130418.83\n",
      "epoch 4850 train loss 91658.24 test loss 130358.914\n",
      "epoch 4860 train loss 91616.98 test loss 130301.25\n",
      "epoch 4870 train loss 91576.055 test loss 130247.46\n",
      "epoch 4880 train loss 91535.42 test loss 130193.414\n",
      "epoch 4890 train loss 91495.11 test loss 130140.016\n",
      "epoch 4900 train loss 91454.66 test loss 130091.41\n",
      "epoch 4910 train loss 91410.9 test loss 130038.24\n",
      "epoch 4920 train loss 91366.0 test loss 129980.164\n",
      "epoch 4930 train loss 91322.27 test loss 129931.66\n",
      "epoch 4940 train loss 91278.9 test loss 129889.8\n",
      "epoch 4950 train loss 91235.79 test loss 129843.38\n",
      "epoch 4960 train loss 91190.44 test loss 129792.75\n",
      "epoch 4970 train loss 91143.99 test loss 129744.34\n",
      "epoch 4980 train loss 91098.15 test loss 129703.47\n",
      "epoch 4990 train loss 91052.836 test loss 129665.66\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_errs = []\n",
    "test_errs = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(5000):\n",
    "        # training is here\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        # Calculate train loss\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        train_errs.append(train_loss)\n",
    "        \n",
    "        # Calculate test loss\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        test_errs.append(test_loss)\n",
    "        \n",
    "        # Print losses\n",
    "        if epoch %10==0:\n",
    "            print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)\n",
    "    pred = sess.run(output, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- R square -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.883066323975672"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate r^2\n",
    "metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12add90b8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12add90f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(train_errs, label='Train loss')\n",
    "plt.plot(test_errs, label=\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#---------------        Mean Squared Error     ------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129625.04588041583"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d9d5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, pred, s=15, alpha=0.5, label=\"experiment vs predicted\", color=\"red\")\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_predicted\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'y_test':[y_test[i][0] for i in range(len(y_test))],\n",
    "                 'pred':[pred[i][0] for i in range(len(pred))]}, \n",
    "                index=[x for x in range(len(pred))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1979.789062</td>\n",
       "      <td>1966.768877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1061.192990</td>\n",
       "      <td>996.142724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>122.500000</td>\n",
       "      <td>28.519741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1251.250000</td>\n",
       "      <td>1221.892273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2148.000000</td>\n",
       "      <td>2136.554199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2808.000000</td>\n",
       "      <td>2744.015442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3851.000000</td>\n",
       "      <td>3774.819580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_test         pred\n",
       "count    64.000000    64.000000\n",
       "mean   1979.789062  1966.768877\n",
       "std    1061.192990   996.142724\n",
       "min     122.500000    28.519741\n",
       "25%    1251.250000  1221.892273\n",
       "50%    2148.000000  2136.554199\n",
       "75%    2808.000000  2744.015442\n",
       "max    3851.000000  3774.819580"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
